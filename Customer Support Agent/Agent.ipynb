{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bayzid03/LangGraph-Hub/blob/main/Customer%20Support%20Agent/Agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Still working on this project"
      ],
      "metadata": {
        "id": "PrTjONdDtPG0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfBgLw5eqLfG"
      },
      "source": [
        "Smart Customer Support Agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4SZQkK1XqLfM"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from typing import TypedDict, Annotated, List\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.runnables.graph import MermaidDrawMethod\n",
        "from IPython.display import display, Image\n",
        "from dotenv import load_dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VfOh8xBIqLfQ"
      },
      "outputs": [],
      "source": [
        "load_dotenv()\n",
        "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
        "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\", temperature=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrEL3ViFqLfS"
      },
      "source": [
        "Defining Agent State and Prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P3ZComYWqLfS"
      },
      "outputs": [],
      "source": [
        "class SupportAgentState(TypedDict):\n",
        "    messages: Annotated[List[HumanMessage | AIMessage], \"The messages in the conversation\"]\n",
        "    user_query: str\n",
        "    query_category: str\n",
        "    solution: str\n",
        "    statisfaction_rating: int\n",
        "    follow_up_needed: bool\n",
        "    support_history: List[str]\n",
        "\n",
        "query_analysis_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"\"\"You are a customer support query analyzer. Analyze the customer query and categorize it into ONE of these categories:\n",
        "\n",
        "Categories:\n",
        "- technical: Software bugs, login issues, app crashes, feature problems, connectivity issues\n",
        "- billing: Payment problems, subscription issues, refunds, charges, account billing\n",
        "- general: Basic questions, account information, how-to guides, product information\n",
        "- escalate: Complex issues, angry customers, legal matters, or anything requiring human intervention\n",
        "\n",
        "Return ONLY the category name (technical/billing/general/escalate) without explanation.\"\"\"),\n",
        "    (\"human\", \"Customer Query: {query}\"),\n",
        "])\n",
        "\n",
        "technical_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"\"\"You are a technical support specialist. Provide clear, step-by-step solutions for technical issues.\n",
        "\n",
        "Format your response as:\n",
        "**Problem Diagnosed:** [Brief description]\n",
        "**Solution Steps:**\n",
        "1. [Step 1]\n",
        "2. [Step 2]\n",
        "3. [Step 3]\n",
        "**Additional Help:** [Any extra tips or resources]\n",
        "\n",
        "Keep solutions practical and easy to follow.\"\"\"),\n",
        "    (\"human\", \"Technical Issue: {query}\"),\n",
        "])\n",
        "\n",
        "billing_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"\"\"You are a billing support specialist. Help customers with payment and subscription issues.\n",
        "\n",
        "Format your response as:\n",
        "**Issue Summary:** [Brief description]\n",
        "**Resolution Steps:**\n",
        "1. [Step 1]\n",
        "2. [Step 2]\n",
        "3. [Step 3]\n",
        "**Next Steps:** [What happens next or follow-up needed]\n",
        "\n",
        "Be empathetic and provide clear billing information.\"\"\"),\n",
        "    (\"human\", \"Billing Issue: {query}\"),\n",
        "])\n",
        "\n",
        "general_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"\"\"You are a friendly customer service representative. Provide helpful information for general inquiries.\n",
        "\n",
        "Format your response as:\n",
        "**Answer:** [Direct answer to their question]\n",
        "**Additional Information:** [Helpful related details]\n",
        "**Resources:** [Where they can find more help]\n",
        "\n",
        "Be friendly, helpful, and informative.\"\"\"),\n",
        "    (\"human\", \"Customer Question: {query}\"),\n",
        "])\n",
        "\n",
        "escalation_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"\"\"You are a customer service manager handling escalated issues. Be professional, empathetic, and assuring.\n",
        "\n",
        "Format your response as:\n",
        "**Acknowledgment:** [Acknowledge their concern]\n",
        "**Next Steps:** [What will happen next]\n",
        "**Timeline:** [When they can expect follow-up]\n",
        "**Contact Information:** [How they can reach human support]\n",
        "\n",
        "Show that their issue is being taken seriously.\"\"\"),\n",
        "    (\"human\", \"Escalated Issue: {query}\"),\n",
        "])\n",
        "\n",
        "satisfaction_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"\"\"You are evaluating customer satisfaction. Based on the solution provided and the original query, rate the likely satisfaction level from 1-5:\n",
        "\n",
        "1 = Very Unsatisfied (complex issue, solution unclear)\n",
        "2 = Unsatisfied (solution partially addresses issue)\n",
        "3 = Neutral (solution is okay but could be better)\n",
        "4 = Satisfied (good solution, addresses main concerns)\n",
        "5 = Very Satisfied (excellent solution, comprehensive help)\n",
        "\n",
        "Return ONLY the number (1-5) without explanation.\"\"\"),\n",
        "    (\"human\", \"Original Query: {query}\\n\\nSolution Provided: {solution}\"),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define Node **Functions**"
      ],
      "metadata": {
        "id": "h32oWjf4rc8B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def input_query(state: SupportAgentState) -> SupportAgentState:\n",
        "    \"\"\"Collect customer's support query\"\"\"\n",
        "    print(\"=== Welcome to Smart Customer Support ===\")\n",
        "    print(\"Please describe your issue or question:\")\n",
        "    user_message = input(\"Your query: \")\n",
        "\n",
        "    return {\n",
        "        **state,\n",
        "        \"user_query\": user_message,\n",
        "        \"messages\": state['messages'] + [HumanMessage(content=user_message)],\n",
        "        \"support_history\": state.get('support_history', []) + [f\"Query: {user_message}\"]\n",
        "    }\n",
        "\n",
        "def analyze_query(state: SupportAgentState) -> SupportAgentState:\n",
        "    \"\"\"Analyze and categorize the customer query using LLM\"\"\"\n",
        "    print(\"ðŸ” Analyzing your query...\")\n",
        "\n",
        "    # Use LLM to categorize the query\n",
        "    response = llm.invoke(query_analysis_prompt.format_messages(query=state['user_query']))\n",
        "    category = response.content.strip().lower()\n",
        "\n",
        "    print(f\"ðŸ“‹ Query categorized as: {category.upper()}\")\n",
        "    print(\"ðŸ”„ Routing to appropriate support specialist...\\n\")\n",
        "\n",
        "    return {\n",
        "        **state,\n",
        "        \"query_category\": category,\n",
        "        \"messages\": state['messages'] + [AIMessage(content=f\"Category: {category}\")],\n",
        "        \"support_history\": state['support_history'] + [f\"Categorized as: {category}\"]\n",
        "    }\n",
        "\n",
        "def technical_support(state: SupportAgentState) -> SupportAgentState:\n",
        "    \"\"\"Handle technical support issues\"\"\"\n",
        "    print(\"ðŸ”§ Technical Support Specialist\")\n",
        "    print(\"=====================================\")\n",
        "\n",
        "    response = llm.invoke(technical_prompt.format_messages(query=state['user_query']))\n",
        "    solution = response.content\n",
        "\n",
        "    print(solution)\n",
        "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
        "\n",
        "    return {\n",
        "        **state,\n",
        "        \"solution\": solution,\n",
        "        \"messages\": state['messages'] + [AIMessage(content=solution)],\n",
        "        \"support_history\": state['support_history'] + [\"Technical solution provided\"]\n",
        "    }"
      ],
      "metadata": {
        "id": "kxpC4PxVrjQe"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}